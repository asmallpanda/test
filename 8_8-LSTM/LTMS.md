一、RNN的局限：长期依赖（Long-TermDependencies）问题

RNN的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果RNN可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。

有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测这句话中“the clouds are in the sky”最后的这个词“sky”，我们并不再需要其他的信息，因为很显然下一个词应该是sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN可以学会使用先前的信息。
但是同样会有一些更加复杂的场景。比如我们试着去预测“I grew up in France...I speak fluent French”最后的词“French”。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的“France”的上下文。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。

不幸的是，在这个间隔不断增大时，RNN会丧失学习到连接如此远的信息的能力。

在理论上，RNN绝对可以处理这样的长期依赖问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN则没法太好的学习到这些知识。Bengio,etal.(1994)等人对该问题进行了深入的研究，他们发现一些使训练RNN变得非常困难的相当根本的原因。

换句话说， RNN 会受到短时记忆的影响。如果一条序列足够长，那它们将很难将信息从较早的时间步传送到后面的时间步。

因此，如果你正在尝试处理一段文本进行预测，RNN 可能从一开始就会遗漏重要信息。在反向传播期间（反向传播是一个很重要的核心议题，本质是通过不断缩小误差去更新权值，从而不断去修正拟合的函数），RNN 会面临梯度消失的问题。

因为梯度是用于更新神经网络的权重值（新的权值 = 旧权值 - 学习率*梯度），**梯度会随着时间的推移不断下降减少，而当梯度值变得非常小时，就不会继续学习**。

换言之，在递归神经网络中，获得小梯度更新的层会停止学习—— 那些通常是较早的层。 由于这些层不学习，**RNN会忘记它在较长序列中以前看到的内容，因此RNN只具有短时记忆**。

而梯度爆炸则是因为计算的难度越来越复杂导致。

然而，幸运的是，有个RNN的变体——LSTM，可以在一定程度上解决梯度消失和梯度爆炸这两个问题！

二、LSTM网络

Long ShortTerm 网络——一般就叫做LSTM——是一种RNN特殊的类型，可以学习长期依赖信息。当然，LSTM和基线RNN并没有特别大的结构不同，但是它们用了不同的函数来计算隐状态。

LSTM的“记忆”我们叫做细胞/cells，你可以直接把它们想做黑盒，这个黑盒的**输入为前状态![h_{t-1}](https://latex.csdn.net/eq?h_%7Bt-1%7D)和当前输入![x_{t}](https://latex.csdn.net/eq?x_%7Bt%7D)**。这些“细胞”会决定哪些之前的信息和状态需要保留/记住，而哪些要被抹去。实际的应用中发现，这种方式可以有效地保存很长时间之前的关联信息。

什么是LSTM网络？

举个例子，当你想在网上购买生活用品时，一般都会查看一下此前已购买该商品用户的评价。

当你浏览评论时，你的大脑下意识地只会记住重要的关键词，比如“amazing”和“awsome”这样的词汇，而不太会关心“this”、“give”、“all”、“should”等字样。如果朋友第二天问你用户评价都说了什么，那你可能不会一字不漏地记住它，而是会说出但大脑里记得的主要观点，比如“下次肯定还会来买”，那其他一些无关紧要的内容自然会从记忆中逐渐消失。
而这基本上就像是 LSTM 或 GRU 所做的那样，它们可以学习只保留相关信息来进行预测，并忘记不相关的数据。简单说，因记忆能力有限，记住重要的，忘记无关紧要的。


LSTM由Hochreiter&Schmidhuber(1997)提出，并在近期被AlexGraves进行了改良和推广。在很多问题，LSTM都取得相当巨大的成功，并得到了广泛的使用。
LSTM通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是LSTM的默认行为，而非需要付出很大代价才能获得的能力！

所有RNN都具有一种重复神经网络模块的链式的形式。在标准的RNN中，这个重复的模块只有一个非常简单的结构，例如一个tanh层。

激活函数 Tanh 作用在于帮助调节流经网络的值，使得数值始终限制在 -1 和 1 之间。

LSTM同样是这样的结构，但是重复的模块拥有一个不同的结构。具体来说，RNN是重复单一的神经网络层，LSTM中的重复模块则包含四个交互的层，三个Sigmoid 和一个tanh层，并以一种非常特殊的方式进行交互。

![img](https://img-blog.csdnimg.cn/img_convert/05ab9341820ee5ca60c5a23252fdd0a6.png)

上图中，σ表示的Sigmoid 激活函数与 tanh 函数类似，不同之处在于 sigmoid 是把值压缩到0~1 之间而不是 -1~1 之间。这样的设置有助于更新或忘记信息：

因为任何数乘以 0 都得 0，这部分信息就会剔除掉；
同样的，任何数乘以 1 都得到它本身，这部分信息就会完美地保存下来
相当于要么是1则记住，要么是0则忘掉，所以还是这个原则：因记忆能力有限，记住重要的，忘记无关紧要的。
此外，对于图中使用的各种元素的图标中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表pointwise的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。

![img](https://img-blog.csdnimg.cn/img_convert/37d708d09eba4de2d07576dd7967fae3.png)

LSTM的核心思想

LSTM的关键就是细胞状态，水平线在图上方贯穿运行。
细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。

![img](https://img-blog.csdnimg.cn/img_convert/873cf8f3a62461edce5103a410dd4c90.png)

LSTM有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个sigmoid神经网络层和一个pointwise乘法的非线性操作。

如此，0代表“不许任何量通过”，1就指“允许任意量通过”！从而使得网络就能了解哪些数据是需要遗忘，哪些数据是需要保存。

![img](https://img-blog.csdnimg.cn/img_convert/108c3707a90d041839cdd83b7bc79971.png)

LSTM拥有三种类型的门结构：遗忘门/忘记门、输入门和输出门，来保护和控制细胞状态。下面，我们来介绍这三个门。

遗忘门/忘记门

在我们LSTM中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为“忘记门”的结构完成。该忘记门会读取上一个输出和当前输入，做一个Sigmoid 的非线性映射，然后输出一个向量（该向量每一个维度的值都在0到1之间，1表示完全保留，0表示完全舍弃，相当于记住了重要的，忘记了无关紧要的），最后与细胞状态相乘。

类比到语言模型的例子中，则是基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语，进而决定丢弃信息。

![img](https://img-blog.csdnimg.cn/img_convert/452fa56a2d04c1bb93732c08b5e6492a.png)

分以下两个步骤理解：

1.对于上图右侧公式中的权值W~f~，准确的说其实是不共享，即是不一样的。有的同学可能第一反应是what？别急，我展开下你可能就瞬间清晰了，即：

![f_{t} = \sigma (W_{fh}h_{t-1} + W_{fx} x_{t} + b_{f})](https://private.codecogs.com/gif.latex?f_%7Bt%7D%20%3D%20%5Csigma%20%28W_%7Bfh%7Dh_%7Bt-1%7D%20&plus;%20W_%7Bfx%7D%20x_%7Bt%7D%20&plus;%20b_%7Bf%7D%29)。
至于右侧公式和左侧的图是怎样的一个一一对应关系呢？如果是用有方向的水流表示计算过程则将一目了然，上动图！红圈表示Sigmoid 激活函数，篮圈表示tanh 函数：

![img](https://img-blog.csdnimg.cn/img_convert/33d34e6909bd9e989fbf7094d97890c4.gif)

输入门

下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分：
第一，sigmoid层称“输入门层”决定什么值我们将要更新；
第二，一个tanh层创建一个新的候选值向量，会被加入到状态中。
下一步，我们会讲这两个信息来产生对状态的更新。

在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语，进而确定更新的信息。


![img](https://img-blog.csdnimg.cn/img_convert/63bc710705b2eecf74fd484c9710ba64.png)

继续分成两个步骤来理解：

1.首先，为便于理解图中右侧的两个公式，我们展开下计算过程，即

![i_{t} = \sigma (W_{ih}h_{t-1} + W_{ix}x_{t} + b_{i})](https://private.codecogs.com/gif.latex?i_%7Bt%7D%20%3D%20%5Csigma%20%28W_%7Bih%7Dh_%7Bt-1%7D%20&plus;%20W_%7Bix%7Dx_%7Bt%7D%20&plus;%20b_%7Bi%7D%29)

![\tilde{C_{t}} = tanh(W_{Ch}h_{t-1} + W_{Cx}x_{t} + b_{C})](https://private.codecogs.com/gif.latex?%5Ctilde%7BC_%7Bt%7D%7D%20%3D%20tanh%28W_%7BCh%7Dh_%7Bt-1%7D%20&plus;%20W_%7BCx%7Dx_%7Bt%7D%20&plus;%20b_%7BC%7D%29)

2.其次，上动图！

![img](https://img-blog.csdnimg.cn/img_convert/cc4a872a1f878a945c6afd912a6e2ff5.gif)

细胞状态

现在是更新旧细胞状态的时间了，C~t-1~更新为C~t~。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。
我们把旧状态与f~t~相乘，丢弃掉我们确定需要丢弃的信息，接着加上i~t~*C~t~。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。
在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方，类似更新细胞状态。
![img](https://img-blog.csdnimg.cn/img_convert/c38a0b8f9bbd4d078d160d3f4d921a34.png)

![img](https://img-blog.csdnimg.cn/img_convert/3c9d64c8ac82b985769cb0903891dabe.gif)

输出门

最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。

首先，我们运行一个sigmoid层来确定细胞状态的哪个部分将输出出去。
接着，我们把细胞状态通过tanh进行处理（得到一个在-1到1之间的值）并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。

在语言模型的例子中，因为他就看到了一个代词，可能需要输出与一个动词相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化，进而输出信息。
![img](https://img-blog.csdnimg.cn/img_convert/a689a85822d6937abfd118a72e46c2e7.png)

依然分两个步骤来理解：

1. 展开图中右侧第一个公式，![o_{t} = \sigma (W_{oh}h_{t-1} + W_{ox}x_{t} + b_{o})](https://private.codecogs.com/gif.latex?o_%7Bt%7D%20%3D%20%5Csigma%20%28W_%7Boh%7Dh_%7Bt-1%7D%20&plus;%20W_%7Box%7Dx_%7Bt%7D%20&plus;%20b_%7Bo%7D%29)

​	2.最后一个动图

![img](https://img-blog.csdnimg.cn/img_convert/fc0d3305d75e0bb3b77602ffc524843e.gif)

LTMS的变体

我们到目前为止都还在介绍正常的LSTM。但是不是所有的LSTM都长成一个样子的。实际上，几乎所有包含LSTM的论文都采用了微小的变体。差异非常小，但是也值得拿出来讲一下。

peephole连接与coupled

其中一个流形的LSTM变体，就是由Gers&Schmidhuber(2000)提出的，增加了“peepholeconnection”。是说，我们让门层也会接受细胞状态的输入。

peephole连接

![img](https://img-blog.csdnimg.cn/img_convert/4deed5dcc4532b43b0428dab6f5c8d61.png)

上面的图例中，我们增加了peephole到每个门上，但是许多论文会加入部分的peephole而非所有都加。

另一个变体是通过使用coupled忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态。
![img](https://img-blog.csdnimg.cn/img_convert/4fa168c3a9a9bbccd930000719757c7a.png)

GRU

另一个改动较大的变体是GatedRecurrentUnit(GRU)，这是由Cho,etal.(2014)提出。它将忘记门和输入门合成了一个单一的更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的LSTM模型要简单，也是非常流行的变体。

![img](https://img-blog.csdnimg.cn/img_convert/d90834b68537f786b8d8c9fe656cf37c.png)

为了便于理解，我把上图右侧中的前三个公式展开一下

1. ![z_{t} = \sigma (W_{zh}h_{t-1} + W_{zx}x_{t})](https://private.codecogs.com/gif.latex?z_%7Bt%7D%20%3D%20%5Csigma%20%28W_%7Bzh%7Dh_%7Bt-1%7D%20&plus;%20W_%7Bzx%7Dx_%7Bt%7D%29)
2. ![r_{t} = \sigma (W_{rh}h_{t-1} + W_{rx}x_{t})](https://private.codecogs.com/gif.latex?r_%7Bt%7D%20%3D%20%5Csigma%20%28W_%7Brh%7Dh_%7Bt-1%7D%20&plus;%20W_%7Brx%7Dx_%7Bt%7D%29)
3. ![\tilde{h} = tanh(W_{rh}(r_{t}h_{t-1}) + W_{x}x_{t})](https://private.codecogs.com/gif.latex?%5Ctilde%7Bh%7D%20%3D%20tanh%28W_%7Brh%7D%28r_%7Bt%7Dh_%7Bt-1%7D%29%20&plus;%20W_%7Bx%7Dx_%7Bt%7D%29)

这里面有个小问题，眼尖的同学可能看到了，z~t~和r~t~都是对h~t-1~、x~t~做的Sigmoid非线性映射，那区别在哪呢？原因在于GRU把忘记门和输入门合二为一了，而是z~t~属于要记住的，反过来1-z~t~则是属于忘记的，相当于对输入h~t-1~、x~t~做了一些更改/变化，而r~t~则相当于先见之明的把输入h~t-1~、x~t~在z~t~/1-z~t~对其做更改/变化之前，先事先存一份原始的，最终在那做一个tanh变化。
这里只是部分流行的LSTM变体。当然还有很多其他的，如Yao,etal.(2015)提出的DepthGatedRNN。还有用一些完全不同的观点来解决长期依赖的问题，如Koutnik,etal.(2014)提出的ClockworkRNN。

要问哪个变体是最好的？其中的差异性真的重要吗？Greff,etal.(2015)给出了流行变体的比较，结论是他们基本上是一样的。Jozefowicz,etal.(2015)则在超过1万种RNN架构上进行了测试，发现一些架构在某些任务上也取得了比LSTM更好的结果。