01 贝叶斯方法

贝叶斯分类

贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯原理为基础，故统称为贝叶斯分类。

先验概率

根据以往经验和分析得到的概率。我们用P(Y)来代表在没有训练数据前假设Y拥有的初始概率。

后验概率

根据已经发生的时间来分析得到的概率。以P(Y|X)代表假设X成立的情况下观察到Y数据的概率，因为它反映了在看到训练数据X后Y成立的置信度。

联合概率

联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。X与Y的联合概率表示为P(X,Y)、P(XY)或P(X∩Y)。

假设X和Y都服从正态分布，那么P(X<5,Y<0)就是一个联合概率，表示X<5,Y<0两个条件同时成立的概率。表示两件事共同发生的概率。

贝叶斯公式
$$
P(Y|X)= {P(X,Y) \over P(X)}={P(X|Y)P(Y) \over P(X)}
$$
P(Y|X)：后验概率	P(X|Y)：似然度	P(Y)：先验概率	P(X)：边际似然度

朴素贝叶斯是典型的生成学习方法。生成学习方法由训练数据学习联合概率分布P(X,Y)，然后求得后验概率分布P(Y|X)。

具体来说，利用训练数据学习P(X|Y)和P(Y)的估计，得到联合概率分布：

P(X,Y)=P(X|Y)P(Y)

02 朴素贝叶斯原理

监督学习方法又分生成方法（Generative approach）和判别方法（Discriminative approach），通过这两种方法所学到的模型分别称为生成模型（Generative Model）和判别模型（Discriminative Model）。

判别模型（Discriminative Model）：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。即：直接估计𝑃(𝑌|𝑋)。

线性回归、逻辑回归、感知机、决策树、支持向量机……

生成模型（Generative Model）：由训练数据学习联合概率分布 𝑃(𝑋, 𝑌)，然后求得后验概率分布𝑃(𝑌|𝑋)。具体来说，利用训练数据学习𝑃(𝑋|𝑌)和𝑃(𝑌)的估计，得到联合概率分布：𝑃(𝑋, 𝑌)＝𝑃(𝑌)𝑃(𝑋|𝑌)，再利用它进行分类。即：估计𝑃(𝑋|𝑌) 然后推导𝑃(𝑌|𝑋)。

朴素贝叶斯、HMM、深度信念网络(DBN)……

1, 朴素贝叶斯是典型的生成式学习方法

生成方法由训练数据学习联合概率分布 𝑃 (𝑋,𝑌)，然后求得后验概率分布𝑃(𝑌|𝑋) 。具体来说，利用训练数据学习𝑃(𝑋|𝑌) 和𝑃(𝑌)的估计，得到联合概率分布：
$$
𝑃(𝑋,𝑌)＝𝑃(𝑌)𝑃(𝑋|𝑌)
$$
概率估计方法可以是极大似然估计或贝叶斯估计。

2, 朴素贝叶斯的基本假设是条件独立性
$$
P(X=x|Y=c_k)=P(x^{(1)},...,x^{(n)}|y^k)=\prod_{j=1}^{n}{P(x^{j}|Y=c_k)}
$$
c~k~代表类别，k代表类别个数。

这是一个较强的假设。由于这一假设，模型包含的条件概率的数量大为减少，朴素贝叶斯法的学习与预测大为简化。因而朴素贝叶斯法高效，且易于实现。其缺点是分类的性能不一定很高。

3, 朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测

我们要求的是𝑃 (𝑌|𝑋)，根据生成模型定义我们可以求𝑃(𝑋|𝑌) 和𝑃(𝑌)假设中的特征是条件独立的。这个称作朴素贝叶斯假设。 形式化表示为，（如果给定𝑍 的情况下，𝑋 和𝑌 条件独立）：
$$
𝑃(𝑋|𝑍)=𝑃(𝑋|𝑌,𝑍)
$$
也乐意表示为：
$$
𝑃(𝑋|𝑌,𝑍)=𝑃(𝑋|𝑍)𝑃(Y|𝑍)
$$
用于文本分类的朴素贝叶斯模型，这个模型称作多值伯努利事件模型。
在这个模型中，我们首先随机选定了邮件的类型𝑝(𝑦)，然后一个人翻阅词典的所有词，随机决定一个词是否出现依照概率p(x^{(1)}^|y)，出现标示为11，否则标示为0 。假设有50000 个单词，那么这封邮件的概率可以表示为：
$$
\begin{align*}
&p(x^{(1)},...,x^{(50000)}|y)\\
&=p(x^{(1)}|y)p(x^{(2)}|y,x^{(1)})p(x^{(3)}|y,x^{(1)},x^{(2)}...p(x^{(50000)}|y,x^{(1)}...,x^{(49999)}\\
&=p(x^{(1)}|y)p(x^{(2)}|y)p(x^{(3)}|y)...p(x^{(50000)}|y)\\
&=\prod_{i=1}^{m}{p(x^{(i)}|y)}\\
\end{align*}
$$
独立性

将输入𝑥 分到后验概率最大的类𝑦 。
$$
y=argmax_{c_{k}}P(Y=c_{k})\prod_{j=1}^{n}{P(X_{j}=x^{(j)}|Y=c_{k})}
$$
后验概率最大等价于0-1损失函数时的期望风险最小化。

X={x~1~,x~2~,...,x~n~},为n维向量的集合

Y={c~1~,c~2~,...,c~k~},K为类别数

训练数据集𝑇={(𝑥~1~，y~1~),(𝑥~2~，y~2~),...,(𝑥~n~，y~n~),} 由𝑃(𝑋,𝑌) 独立同分布产生。

朴素贝叶斯法对条件概率分布作了条件独立性的假设。由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体地，条件独立性假设是：
$$
\begin{align}
P(X=x|Y=c_{k}) &=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_{k}\\
&=\prod_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k}}
\end{align}{(1)}
$$
朴素贝叶斯法分类时，对给定的输入𝑥 ，通过学习到的模型计算后验概率分布𝑃(𝑌=𝑐~𝑘~|𝑋=𝑥），将后验概率最大的类作为𝑥 的类输出。根据贝叶斯定理：
$$
P(Y|X) = {P(X|Y)P(Y) \over P(X)}
$$
可以计算后验概率
$$
P(Y=c_{k}|X=x)={P(X=x|Y=c_{k})P(Y=c_{k}) \over \sum_{k=1}^{k}{P(X=x|Y=c_{k})P(Y=c_{k})}}{(2)}
$$
将式 (1)代入公式 (2)，可以得到
$$
P(Y=c_{k}|X=x)={\prod_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k})P(Y=c_{k})} \over \sum_{k=1}^{k}\prod_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k})P(Y=c_{k})}}
$$
贝叶斯分类器可以表示为：
$$
y=f(x)=argmax_{c_{k}}{\prod_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k})P(Y=c_{k})} \over \sum_{k=1}^{k}\prod_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k})P(Y=c_{k})}}
$$
上式中分母中𝑐~𝑘~都是一样的，即不会对结果产生影响，即
$$
y=f(x)=argmax_{c_{k}}\prod_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_{k})P(Y=c_{k})}
$$
