TF-IDF

1.TF-IDF算法介绍

**TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）**是一种用于信息检索（information retrieval）与文本挖掘（text mining）的常用**加权技术**。

TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。**字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。**

**TF-IDF的主要思想是**：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

**当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。**

**（1）TF是词频(Term Frequency)**

**词频（TF）** **表示词条（关键字）在文本中出现的频率**。

这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。

公式：
$$
tf_{ij}={{n_{i,j}}\over{\sum_{k}{n_{k,j}}}}
$$
即：
$$
TF_w = {{在某一类中词条w出现的次数}\over {该类中所有的词条数目}}
$$
其中 **ni,j** 是该词在文件 **dj** 中出现的次数，分母则是文件 dj 中所有词汇出现的次数总和；

**（2） IDF是逆向文件频率(Inverse Document Frequency)**

**逆向文件频率 (IDF)** ：某一特定词语的IDF，可以由**总文件数目除以包含该词语的文件的数目**，**再将得到的商取对数得到**。

如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。

公式：
$$
idf_i=log{|D|\over {|\begin{Bmatrix} j:t_i∈d_j \end{Bmatrix}|+1}}
$$
其中，**|D|** **是语料库中的文件总数**。 **|{j:ti∈dj}| 表示包含词语 ti 的文件数目**（即 ni,j≠0 的文件数目）。如果该词语不在语料库中，就会导致分母为零，因此**一般情况下使用 1+|{j:ti∈dj}|**

即：
$$
IDF=log({语料库的文档总数 \over {包含词条w的文档数 + 1}})，分母之所以要加1是为了避免坟墓为0
$$
**（3）TF-IDF实际上是：TF \* IDF**

某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。

公式：
$$
TF-IDF=TF*IDF
$$


**注：** TF-IDF算法非常容易理解，并且很容易实现，但是其简单结构并没有考虑词语的语义信息，无法处理一词多义与一义多词的情况。

2.TF-IDF算法步骤

第一步，计算词频

![img](https://pic4.zhimg.com/80/v2-281a550de928afe343c055d06371cf77_720w.webp)

考虑到文章有长短之分，为了便于不同文章的比较，进行"词频"标准化。

![img](https://pic2.zhimg.com/v2-393435b342546a2f1736d1d755adb1cd_r.jpg)

第二步，计算逆文档频率：

这时，需要一个语料库（corpus），用来模拟语言的使用环境。

![img](https://pic2.zhimg.com/80/v2-1d5c436e04f497544d72fec6909a3fad_720w.webp)

如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。

第三步，计算TF-IDF：

![img](https://pic3.zhimg.com/80/v2-5560a4b2efa3330021b8b2ef13a471fe_720w.webp)

可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。所以，自动提取关键词的算法就很清楚了，就是**计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。**

优缺点：

TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用**词频**来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。如果要体现词的上下文结构，那么你可能需要使用word2vec算法来支持。