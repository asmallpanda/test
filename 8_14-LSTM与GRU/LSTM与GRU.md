LSTM和GRU

一、LSTM

1.什么是循环神经网络LSTM？

LSTM指的是长短期记忆网络（Long Short Term Memory），它是循环神经网络中最知名和成功的扩展。由于循环神经网络有梯度消失和梯度爆炸的问题，学习能力有限，在实际任务中的效果很难达到预期。为了增强循环神经网络的学习能力，缓解网络的梯度消失等问题，LSTM神经网络便横空出世了。LSTM可以对有价值的信息进行长期记忆，不仅减小循环神经网络的学习难度，并且由于其增加了长时记忆，在一定程度上也缓解了梯度消失的问题。（注：梯度爆炸的问题用一般的梯度裁剪方法就可以解决）。
2.LSTM实现长短期记忆的原理是什么？

LSTM网络内部结构示意图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190908174524547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzM2NjE4NjYw,size_16,color_FFFFFF,t_70)

 与传统的循环神经网络相比，LSTM在h~t-1~，x~t~的基础上加了一个cell state（即长时记忆状态c~t−1~来计算h~t~，并且对网络模型内部进行了更加精心的设计，加入了遗忘门f~t~、输入门i~t~、输出门o~t~，三个门神经元以及一个内部记忆神经元c~t~（上图2.1中的LSTM网络模型内部黄色的σ从左到右分别指的是f~t~、i~t~、o~t~ ，黄色的tanh则指的是c~t~。遗忘门神经元f~t~控制前一步记忆单元中信息有多大程度上被遗忘掉；输入门神经元i~t~控制当前记忆中的信息以多大程度更新到记忆单元中；输出门神经元o~t~控制当前的hidden state（即短时记忆单元）的输出有多大程度取决于当前的长时记忆单元。在一个训练好的网络中，当输入的序列没有重要信息时，LSTM遗忘门的值接近于1，输入门的值接近于0，此时过去的信息将会保留，从而实现了长时记忆的功能；当输入的序列中出现了重要信息时，LSTM应当将其存入记忆中，此时其输入门的值会接近于1；当输入的序列中出现了重要信息，且该信息意味着之前的记忆不再重要时，则输入门的值会接近于1，而遗忘门的值会接近于0，这样旧的记忆就会遗忘，新的重要信息被记忆。经过这样的设计，整个网络更容易学习到序列之间的长期依赖。

3.LSTM的原理公式是什么？

经典的LSTM中，第t步的更新计算公式如下（⊙表示元素点乘）：
$$
\begin{cases}
输入：x_{input}=concat[h_{t-1}, x_t]\\
遗忘门神经元：f_t=σ(x_{input}W_f+b_f)\\
遗忘后的长时记忆：c^{'}_{t-1}=f_t⊙c_{t-1}\\
记忆门神经元：c_t=tanh(x_{input}W_c+b_c)\\
输入门神经元：i_t=σ(x_{input}W_i+b_i)\\
输入后的记忆：c^{'}_{t}=i_t⊙c_{t}\\
t时刻的长时记忆：c_t=c^{'}_{t-1}+c^{'}_{t}\\
输出门的神经元：o_t=σ(x_{input}W_o+b_o)\\
t时刻的短时记忆：h_{t}=o_t⊙tanh(c_{t})
\end{cases}
$$
其中输入x ~input~是通过对上一时刻t−1的短期记忆状态h_{t-1}以及当前时刻t的字（词）向量输入x_t 进行特征维度的concat所获得的。σ指的是sigmoid函数，遗忘门、输入门以及输出门神经元的输出结果是向量，由于三个门神经元均采用sigmoid作为激活函数，所以输出向量的每个元素均在0~1之间，用于控制各维度流过阀门的信息量；记忆门神经元的输出结果仍为向量，且与遗忘门、输入门以及输出门神经元的输出向量维度等同，由于记忆门神经元使用的激活函数是 tanh，所以其输出向量的每个元素在-1~1之间。

W~f~， b~f~，W~i~，b~i~，W~c~，b~c~，W~o~，b~o~是各个门神经元的参数，是在训练过程中需要学习得到的。与传统的循环神经网络所不同的是，LSTM循环神经网络新增了长时记忆单元，  从上一个长时记忆单元的状态c ~t−1~到当前的长时记忆单元c~t~不仅取决于激活函数tanh计算得到的状态，而且还由输入门和遗忘门神经元来共同控制。

4.LSTM为什么选择sigmoid作为遗忘门、输入门以及输出门神经元的激活函数，又为什么选择tanh作为记忆门神经元的激活函数?

我们可以注意到，无论是sigmoid还是tanh，它们均属于饱和函数，也就是说当输入达到一定值的情况下，输出就不会明显变化了。比如，当输入小于一定值时，sigmoid输出几乎接近于0，tanh ⁡ 输出几乎接近于-1；当输入大于一定值时，sigmoid，tanh输出均接近于1。
(1). 如果使用非饱和的激活函数，例如relu，我们很难实现门控效果。sogmoid的函数输出在0~1之间，且输入较大或较小时，其输出会非常接近1或0，从而保证该门是开或关。故选择sigmoid作为遗忘门、输入门以及输出门神经元的激活函数最合适不过。
(2). tanh函数的输出在-1~1之间，并且中心为0，这与大多数场景下特征分布是0中心吻合，并且，tanh 在输入为0附近的时相比sigmoid函数有更大的梯度。通常使模型收敛更快。

5.常见问题

（1）LSTM有几个输入几个输出？

三个输入：序列中的字（词）向量输入x~t~、短时记忆状态输入h~t−1~以及长时记忆状态输入c~t−1~。

两个输出：短时记忆状态输出h~t~和长时记忆状态输出c~t~。

（2）LSTM有几个神经元，分别使用什么激活函数?

四个神经元：遗忘门、输入门、输出门以及记忆门神经元，其中遗忘门、输入门以及输出门神经元使用sigmoid作为激活函数，实现门控效果；记忆门神经元使用tanh作为激活函数，来吻合0中心特征分布，并且tanh函数在0中心有较大梯度，提高模型收敛速度。

6.如何构建双向LSTM?

双向LSTM结构示意图:

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190908204450160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzM2NjE4NjYw,size_16,color_FFFFFF,t_70)

(1). 初始化两个新的LSTM，一个LSTM将序列正序输入，另一个LSTM将序列反序输入,然后将对应字（词）向量的LSTM输出的结果进行concat，并将concat的结果作为该字（词）向量的新特征向量。
注：这个concat的输入，既可以说是LSTM网络结果输出，也可以说是LSTM短时记忆状态输出。若考虑长时记忆状态的concat，则应是两个序列的最后一个长时记忆状态进行concat，详细见（3）总结

(2). Example：假如有一个序列为"大梦想家"，我们正序输入即将"大"，“梦”，“想”，“家"的字向量按照顺序依次输入到第一个LSTM神经网络，与此同时将"家”，“想”，“梦”，"大"的字向量按照顺序依次输入到第二个LSTM神经网络，然后将得到的"大"对应的第一个LSTM向量输出与"大"对应的第二个LSTM向量的输出进行concat。

(3). 总结
1.在上面的Example中，我们不难看出，对于正序输入的"大"字向量来说，其输出并没有其他字向量的信息，而对于反序输入的"大"字向量来说，其输出包含了"家"，“想”，"梦"所有在反序中"大"字向量前面的字向量信息。将这两个"大"所得到的LSTM网络输出进行concat，于是"大"字向量获得了concat后的新特征向量，该特征向量拥有了序列的全局信息，即拥有全局视野。
2.对于双向LSTM来说，若我们考虑将第一个LSTM在正序序列输入的情况下，所输出的最后一个长时记忆状态，即对应的"家"的输出状态与第二个LSTM在反序序列输入的情况下，所输出的最后一个长时记忆状态，即对应的"大"的输出状态进行concat，这样所得到的hidden_state以及cell_state将具有更持久的记忆，即记忆能力更强。

二、GRU

1.什么是循环神经网络GRU？

GRU指的是门控循环单元（Gated Recurrent Units ），它是循环神经网络中的一种门控机制，是由Kyunghyun Cho等人于2014年引入的，它与具有遗忘门的长短期记忆网络（LSTM）相类似。

2.GRU实现记忆的原理是什么？

GRU网络原理示意图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190913110704479.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzM2NjE4NjYw,size_16,color_FFFFFF,t_70#pic_center)

3.GRU的原理公式是什么？

经典的GRU中，第t步的更新计算公式如下（⊙表示元素点乘）：
$$
\begin{cases}
输入：x_{input}=concat[h_{t-1}, x_t]\\
重置门神经元：r_t=σ(x_{input}W_r+b_r)\\
记忆门神经元：h_t=tanh([r_t⊙h_{t-1},x_{input}]W_h+b_h)\\
输入门神经元：z_t=σ(x_{input}W_z+b_z)\\
输入后的记忆：h^{'}_{t}=z_t⊙h_{t}\\
遗忘门神经元：f_t=1-z_t\\
遗忘后的t-1时刻记忆：h^{'}_{t-1}=f_t⊙h_{t-1}\\
t时刻的记忆：h_{t}=h^{'}_{t-1}+h^{'}_{t}
\end{cases}
$$
其中输入x~input~是通过对上一时刻t − 1的记忆状态h~t−1~以及当前时刻t的字（词向量输入x~t~进行特征维度的concat所获得的。σ指的是sigmoid函数，重置门神经元r~t~和输入门神经元z~t~的输出结果是向量，由于两个门神经元均采用sigmoid作为激活函数，所以输出向量的每个元素均在0-1之间，用于控制各维度流过阀门的信息量；记忆门神经元h~t~的输出结果仍为向量，且与重置门和输入门神经元的输出向量维度等同，由于记忆门神经元使用的激活函数是tanh，所以其输出向量的每个元素均在-1~1之间。

W~r~， b~r~，W~z~，b~z~，W~h~，b~h~是各个门神经元的参数，是要在训练过程中学习得到的。

4.GRU与LSTM的比较

（1）GRU相比于LSTM少了输出门，其参数比LSTM少。

（2）GRU在复调音乐建模和语音信号建模等特定任务上的性能和LSTM差不多，在某些较小的数据集上，GRU相比于LSTM表现出更好的性能。

（3）LSTM比GRU严格来说更强，因为它可以很容易地进行无限计数，而GRU却不能。这就是GRU不能学习简单语言的原因，而这些语言是LSTM可以学习的。

（4）GRU网络在首次大规模的神经网络机器翻译的结构变化分析中，性能始终不如LSTM。